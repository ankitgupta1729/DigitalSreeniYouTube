{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeYg2QSdXUQQo3flvHQ6EF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/339_surrogate_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/6GxFAw2IFyw"
      ],
      "metadata": {
        "id": "V2p8H3vbtoSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surrogate optimization is a method used to solve optimization problems that are expensive or time-consuming to evaluate directly. It relies on constructing a surrogate model (also known as a metamodel) that approximates the objective function based on a limited number of evaluations. The surrogate model is then used to guide the search for the optimal solution. This approach is particularly useful when dealing with complex simulations, physical experiments, or other computationally expensive tasks.\n",
        "<p>\n",
        "Unlike traditional metaheuristic approaches like Particle Swarm Optimization (PSO) and Simulated Annealing (SA), surrogate optimization is not strictly a metaheuristic. It can be combined with metaheuristics or other optimization techniques to enhance their efficiency.\n",
        "<p>\n",
        "Surrogate optimization is often considered a Bayesian approach because it incorporates Bayesian principles in its methodology, particularly through the use of Gaussian Processes (GPs) and Bayesian optimization techniques.\n",
        "<p>\n",
        "To get a practical understanding, let's implement a simple example of surrogate optimization using the same objective function from our PSO tutorial. We'll use a popular surrogate model called Gaussian Process (GP) and the Bayesian Optimization framework."
      ],
      "metadata": {
        "id": "aCUxI0Vlabnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ur4EZVqub5FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function (Simple function with a solution of, [4, 5, -6])\n",
        "def objective_function(params):\n",
        "    x, y, z = params[0], params[1], params[2]\n",
        "    return (x-4)**2 + (y-5)**2 + (z+6)**2\n",
        "\n",
        "# Define the bounds of the search space\n",
        "bounds = np.array([[-10, -10, -10], [10, 10, 10]])\n"
      ],
      "metadata": {
        "id": "3IXjBdJ_b4ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly sample a few initial points in the search space and evaluate the objective function at these points. We use a Gaussian Process model to approximate the objective function. The Matern kernel is commonly used in surrogate optimization. <p>\n"
      ],
      "metadata": {
        "id": "4i37AijQb9Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample initial points\n",
        "n_initial_points = 5\n",
        "initial_points = np.random.uniform(low=bounds[0], high=bounds[1], size=(n_initial_points, 3))\n",
        "initial_values = np.array([objective_function(p) for p in initial_points])\n",
        "print(\"Initial points are: \", initial_points)\n",
        "print(\"Initial values for the objective function are: \", initial_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psKIqfwGcBs2",
        "outputId": "fed11d04-8fe0-4016-fb3c-38d862235fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial points are:  [[-4.68611508 -5.81172591  4.38565136]\n",
            " [ 3.69743208  0.93772233  7.47319329]\n",
            " [-0.65339392 -4.81721076 -1.74069166]\n",
            " [ 0.75507018 -8.10809832  8.81978001]\n",
            " [-8.68497866 -9.24062617 -6.79973467]]\n",
            "Initial values for the objective function are:  [300.20376651 198.12058467 136.17340963 401.97769056 364.34369271]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Gaussian Process (GP) is a powerful statistical model used for regression tasks. It is particularly well-suited for surrogate modeling in optimization because it provides a probabilistic prediction, which includes both a mean prediction and an uncertainty estimate. This property is essential for making decisions about where to sample next in the search space. In summary, it helps estimate the objective function at unobserved points. <p>\n",
        "The Matern kernel is a popular choice for the covariance function (or kernel) in Gaussian Processes. It is more flexible than the Radial Basis Function (RBF) kernel because it has an additional parameter, nu, which controls the smoothness of the function. The parameter nu affects the smoothness of the predicted function: smaller values of nu lead to rougher functions, while larger values lead to smoother functions.\n",
        "<br> nu=1.5 or nu=2.5: Commonly used values that provide a good balance between smoothness and flexibility."
      ],
      "metadata": {
        "id": "LsoFrvr9uWp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the initial Gaussian Process model\n",
        "kernel = Matern(nu=2.5)  #Create a Matern kernel\n",
        "\n",
        "#Initialize the GP model with the specified Matern kernel and other parameters.\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-6)\n",
        "gp.fit(initial_points, initial_values) #Train the GP model on the initial set of points and their corresponding objective function values."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "2YYcahbsuA3P",
        "outputId": "22556305-1697-4435-9162-177a04a8ae40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianProcessRegressor(alpha=1e-06, kernel=Matern(length_scale=1, nu=2.5),\n",
              "                         n_restarts_optimizer=10)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianProcessRegressor(alpha=1e-06, kernel=Matern(length_scale=1, nu=2.5),\n",
              "                         n_restarts_optimizer=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianProcessRegressor</label><div class=\"sk-toggleable__content\"><pre>GaussianProcessRegressor(alpha=1e-06, kernel=Matern(length_scale=1, nu=2.5),\n",
              "                         n_restarts_optimizer=10)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The acquisition function guides the search for the next point to evaluate. We use the Expected Improvement (EI) criterion, which balances exploration and exploitation. <p>\n",
        "The acquisition function's role is to guide the search for the optimal solution by determining which point in the search space should be evaluated next. The acquisition function balances exploration (searching new areas) and exploitation (refining known good areas)."
      ],
      "metadata": {
        "id": "dNs7immwcPum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the acquisition function (Expected Improvement)\n",
        "# Note that norm.cdf(Z) is the cumulative distribution function of the standard normal distribution,\n",
        "# representing the probability that the improvement will be achieved.\n",
        "# norm.pdf(Z) is the probability density function of the standard normal distribution,\n",
        "# accounting for the uncertainty in the prediction.\n",
        "# More information? https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html\n",
        "def acquisition_function(x, gp, y_min):\n",
        "    mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)  # Predicted Mean (mu) and Standard Deviation (sigma)\n",
        "    with np.errstate(divide='warn'):    #  warning will be issued whenever a division by zero occurs during the execution of the code block\n",
        "        imp = y_min - mu  #improvement: If imp is positive, it indicates that the point x has the potential to improve upon the best known value.\n",
        "        Z = imp / sigma  # Standardized improvement, normalized by the standard deviation to account for the uncertainty in the prediction\n",
        "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z) # Expected Improvement, combine the improvement and the probability of achieving that improvement.\n",
        "        ei[sigma == 0.0] = 0.0  # If sigma is zero, meaning the prediction is certain, the expected improvement is set to zero to avoid division by zero.\n",
        "    return -ei\n",
        "\n",
        "# Perform the optimization:\n",
        "# First, initialize the Gaussian Process (GP) model with some initial points and their corresponding objective function values.\n",
        "# Then, optimize by iterating for a specified number of iterations to perform:\n",
        "    # Candidate selection\n",
        "    # Evaluate the objective function\n",
        "    # Update the GP model\n",
        "n_iterations = 50\n",
        "for i in range(n_iterations):\n",
        "    # Find the next point to evaluate using multiple starting points for robustness\n",
        "    candidates = np.random.uniform(low=bounds[0], high=bounds[1], size=(10, 3))  # 10 random candidates\n",
        "\n",
        "    # optimization of the acquisition function for each candidate point in the search space\n",
        "    # and store the optimization results (minimum point and function value)  - in the res_list.\n",
        "    # To find the minimum of the objective function, let us use L-BGFS-B\n",
        "    # 'L-BFGS-B' refers to Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm\n",
        "    # with box constraints, which is suitable for optimization problems with bounds\n",
        "    res_list = [minimize(lambda x: acquisition_function(x, gp, initial_values.min()),\n",
        "                         x0=candidate, bounds=bounds.T, method='L-BFGS-B') for candidate in candidates]\n",
        "    res = min(res_list, key=lambda x: x.fun)\n",
        "    next_point = res.x\n",
        "\n",
        "    # Evaluate the objective function at the new point\n",
        "    next_value = objective_function(next_point)\n",
        "\n",
        "    # Update the Gaussian Process model with the new point\n",
        "    initial_points = np.vstack((initial_points, next_point))\n",
        "    initial_values = np.append(initial_values, next_value)\n",
        "    gp.fit(initial_points, initial_values)\n",
        "\n",
        "    # Print the progress\n",
        "    print(f'Iteration {i+1}: Best Cost = {initial_values.min():.6f}')\n",
        "\n",
        "# Print the results\n",
        "best_index = np.argmin(initial_values)\n",
        "global_best_position = initial_points[best_index]\n",
        "global_best_cost = initial_values[best_index]\n",
        "\n",
        "print('Global Best Position:', global_best_position)\n",
        "print('Global Best Cost:', global_best_cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bryeJF_Vbch1",
        "outputId": "71d5c399-e72d-4469-cca8-f12bef91ada0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Best Cost = 77.000000\n",
            "Iteration 2: Best Cost = 77.000000\n",
            "Iteration 3: Best Cost = 77.000000\n",
            "Iteration 4: Best Cost = 77.000000\n",
            "Iteration 5: Best Cost = 77.000000\n",
            "Iteration 6: Best Cost = 17.765558\n",
            "Iteration 7: Best Cost = 11.313780\n",
            "Iteration 8: Best Cost = 11.313780\n",
            "Iteration 9: Best Cost = 11.313780\n",
            "Iteration 10: Best Cost = 11.313780\n",
            "Iteration 11: Best Cost = 0.011491\n",
            "Iteration 12: Best Cost = 0.011491\n",
            "Iteration 13: Best Cost = 0.011491\n",
            "Iteration 14: Best Cost = 0.011491\n",
            "Iteration 15: Best Cost = 0.011491\n",
            "Iteration 16: Best Cost = 0.011491\n",
            "Iteration 17: Best Cost = 0.011491\n",
            "Iteration 18: Best Cost = 0.011491\n",
            "Iteration 19: Best Cost = 0.011491\n",
            "Iteration 20: Best Cost = 0.011491\n",
            "Iteration 21: Best Cost = 0.011491\n",
            "Iteration 22: Best Cost = 0.011491\n",
            "Iteration 23: Best Cost = 0.011491\n",
            "Iteration 24: Best Cost = 0.011491\n",
            "Iteration 25: Best Cost = 0.011491\n",
            "Iteration 26: Best Cost = 0.011491\n",
            "Iteration 27: Best Cost = 0.011491\n",
            "Iteration 28: Best Cost = 0.011491\n",
            "Iteration 29: Best Cost = 0.011491\n",
            "Iteration 30: Best Cost = 0.011491\n",
            "Iteration 31: Best Cost = 0.011491\n",
            "Iteration 32: Best Cost = 0.011491\n",
            "Iteration 33: Best Cost = 0.011491\n",
            "Iteration 34: Best Cost = 0.011491\n",
            "Iteration 35: Best Cost = 0.011491\n",
            "Iteration 36: Best Cost = 0.011491\n",
            "Iteration 37: Best Cost = 0.011491\n",
            "Iteration 38: Best Cost = 0.011491\n",
            "Iteration 39: Best Cost = 0.011491\n",
            "Iteration 40: Best Cost = 0.011491\n",
            "Iteration 41: Best Cost = 0.011491\n",
            "Iteration 42: Best Cost = 0.011491\n",
            "Iteration 43: Best Cost = 0.011491\n",
            "Iteration 44: Best Cost = 0.011491\n",
            "Iteration 45: Best Cost = 0.011491\n",
            "Iteration 46: Best Cost = 0.011491\n",
            "Iteration 47: Best Cost = 0.011491\n",
            "Iteration 48: Best Cost = 0.011491\n",
            "Iteration 49: Best Cost = 0.011491\n",
            "Iteration 50: Best Cost = 0.011491\n",
            "Global Best Position: [ 4.04718404  5.04480646 -5.91481179]\n",
            "Global Best Cost: 0.011490983716783096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you understand the inner workings of the surrogate optimization, let us use a library that offers this optimization out of the box."
      ],
      "metadata": {
        "id": "7nUv52Zp02VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMEbOWIo0oGj",
        "outputId": "7dbf6dc6-d233-4264-b2be-c22d41049da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.4.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize\n",
        "from skopt.learning import GaussianProcessRegressor\n",
        "from skopt.learning.gaussian_process.kernels import Matern\n",
        "from skopt.acquisition import gaussian_ei\n",
        "import numpy as np\n",
        "\n",
        "# Define the objective function\n",
        "def objective_function(params):\n",
        "    x, y, z = params[0], params[1], params[2]\n",
        "    return (x-4)**2 + (y-5)**2 + (z+6)**2\n",
        "\n",
        "# Define the bounds of the search space\n",
        "bounds = [(-10, 10), (-10, 10), (-10, 10)]\n",
        "\n",
        "# Define the initial points and values\n",
        "initial_points = []\n",
        "initial_values = []\n",
        "\n",
        "# Define the GP model\n",
        "kernel = Matern(nu=2.5)\n",
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)\n",
        "\n",
        "# Perform surrogate optimization using gp_minimize\n",
        "res = gp_minimize(objective_function, bounds, acq_func=\"EI\", n_calls=30, n_random_starts=10, random_state=42)\n",
        "\n",
        "# Print the results\n",
        "print('Global Best Position:', res.x)\n",
        "print('Global Best Cost:', res.fun)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pZZvhy30iLE",
        "outputId": "52c9ec58-0ba1-4d27-b37e-aa053a4dee0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [-4, 6, 7]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [-5, -9, -4]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [-4, -5, -9]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [10, -7, -8]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [0, -7, -10]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [7, -4, 7]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [1, 4, -7]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [-5, 5, 7]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [7, -7, 10]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [-3, 8, 1]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [7, -8, 5]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [4, 5, -6] before, using random point [8, 9, 8]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Best Position: [4, 5, -6]\n",
            "Global Best Cost: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a more complex objective function for demonstration purposes. We'll use the Rosenbrock function, which is a commonly used benchmark function in optimization. The Rosenbrock function is defined as: <p>\n",
        "\n",
        "f(x,y)=(a−x)<sup>2</sup> + b(y−x<sup>2</sup>)<sup>2</sup>\n",
        "<p>\n",
        "where a and b are constants, typically a=1 and b=100. <p>\n",
        "\n",
        "It has a global minimum at x=a and y=a<sup>2</sup> <p>\n",
        "<p> Let us define bounds to be (-5, 5), (-5, 5) in which case a spossible solution would be x=1 and y=1. <p>\n",
        "Then, change the bounds to (2, 5), (-5, 5). You should see the solution to be x=2 and y=4."
      ],
      "metadata": {
        "id": "KCcT2DOd12oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize\n",
        "from skopt.learning import GaussianProcessRegressor\n",
        "from skopt.learning.gaussian_process.kernels import Matern\n",
        "import numpy as np\n",
        "\n",
        "# Define the Rosenbrock function\n",
        "def rosenbrock(x):\n",
        "    a = 1\n",
        "    b = 100\n",
        "    return (a - x[0])**2 + b * (x[1] - x[0]**2)**2\n",
        "\n",
        "# Define the bounds of the search space\n",
        "bounds = [(2, 5), (-5, 5)]\n",
        "\n",
        "# Define the GP model\n",
        "kernel = Matern(nu=2.5)\n",
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)\n",
        "\n",
        "# Perform surrogate optimization using gp_minimize\n",
        "res = gp_minimize(rosenbrock, bounds, n_calls=30, n_random_starts=10, random_state=42)\n",
        "\n",
        "# Print the results\n",
        "print('Global Best Position:', res.x)\n",
        "print('Global Best Cost:', res.fun)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE4TMmG_1h27",
        "outputId": "3093d0cc-a2fb-4e63-cba8-e683ba7b5e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [3, -1]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, 1]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, 3]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [3, -1]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, -4]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, 5]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, 1]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, 2]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [2, 1]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [5, 4]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [3, 5]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [5, 3]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, -1]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [2, 5] before, using random point [4, 0]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Best Position: [2, 4]\n",
            "Global Best Cost: 1\n"
          ]
        }
      ]
    }
  ]
}